{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["LyjoYET04BgB"]},"gpuClass":"premium","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Capstone Project Carlos Figueroa\n","metadata":{"id":"ruU--OBCwQwD"}},{"cell_type":"markdown","source":"Load the packages","metadata":{"id":"BWuaaI1YwZBk"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nimport cv2\nimport os\nimport numpy as np\nimport pickle as pkl\nimport torch\nfrom torchvision.utils import save_image","metadata":{"id":"hLeFYSbfwYY3","execution":{"iopub.status.busy":"2022-12-14T18:33:53.408575Z","iopub.execute_input":"2022-12-14T18:33:53.410074Z","iopub.status.idle":"2022-12-14T18:33:56.257853Z","shell.execute_reply.started":"2022-12-14T18:33:53.409936Z","shell.execute_reply":"2022-12-14T18:33:56.256342Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Function to translate data\n","metadata":{"id":"a8DO19cUwbpu"}},{"cell_type":"code","source":"class LazyLoadDataset(Dataset):\n    def __init__(self,path,train=True,transform=None):\n        self.transform = transform\n        self.train = train\n        path=path+(\"train/\" if train else \"test/\")\n        \n        self.pathX=path+\"X/\"\n        self.pathY=path+\"Y/\"\n        \n        self.data=os.listdir(self.pathX)\n        \n    def __getitem__(self,idx):\n        f=self.data[idx]\n        \n        #X\n        #read rgb images\n        img0=cv2.imread(self.pathX+f+\"/rgb/0.png\")\n        img1=cv2.imread(self.pathX+f+\"/rgb/1.png\")\n        img2=cv2.imread(self.pathX+f+\"/rgb/2.png\")\n        if self.transform is not None:\n            img0=self.transform(img0).numpy().T\n            img1=self.transform(img1).numpy().T\n            img2=self.transform(img2).numpy().T\n        #read image depth\n        depth=np.load(self.pathX+f+\"/depth.npy\")/1000\n        \n        #read field ID\n        field_id=pkl.load(open(self.pathX+f+\"/field_id.pkl\",\"rb\"))\n        #Y\n        if self.train:\n            Y=np.load(self.pathY+f+\".npy\")\n        \n        #normalize rgb 0-255\n        \n        img0=cv2.normalize(img0, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX)\n        img1=cv2.normalize(img1, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX)\n        img2=cv2.normalize(img2, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX)\n        depth=cv2.normalize(depth/1000, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX)   \n        \n        if self.train:\n            return (img0,img1,img2,depth,int(field_id)), Y\n        else: \n            return (img0,img1,img2,depth,int(field_id))\n            \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T18:34:00.005368Z","iopub.execute_input":"2022-12-14T18:34:00.007034Z","iopub.status.idle":"2022-12-14T18:34:00.029072Z","shell.execute_reply.started":"2022-12-14T18:34:00.006958Z","shell.execute_reply":"2022-12-14T18:34:00.027374Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pickle as pkl\nimport torch\nfrom torchvision.utils import save_image\n\ntrainx = torch.load('/kaggle/input/csci-ua-473-intro-to-machine-learning-fall22/train/train/trainX.pt')\ntrainy = torch.load('/kaggle/input/csci-ua-473-intro-to-machine-learning-fall22/train/train/trainY.pt')\ntestx = torch.load('/kaggle/input/csci-ua-473-intro-to-machine-learning-fall22/test/test/testX.pt')\n\nnum_train = trainx[0].shape[0]\nnum_test = testx[0].shape[0]\n\nos.makedirs('./lazydata', exist_ok=True)\n\n# Save train data\nos.makedirs('./lazydata/train', exist_ok=True)\nos.makedirs('./lazydata/train/X', exist_ok=True)\nos.makedirs('./lazydata/train/Y', exist_ok=True)\nfor i in range(num_train):\n    os.makedirs('./lazydata/train/X/{}'.format(i), exist_ok=True)\n    # rgb\n    os.makedirs('./lazydata/train/X/{}/rgb'.format(i), exist_ok=True)\n    for j in range(3):\n        save_image(trainx[0][i][j]/255, './lazydata/train/X/{}/rgb/{}.png'.format(i, j))\n    # depth\n    depth = trainx[1][i].numpy()\n    np.save('./lazydata/train/X/{}/depth.npy'.format(i), depth)\n    # field id\n    pkl.dump(trainx[2][i], open('./lazydata/train/X/{}/field_id.pkl'.format(i), 'wb'))\n\n    y = trainy[0][i].numpy()\n    np.save('./lazydata/train/Y/{}.npy'.format(i), y)\nprint(\"Saved train data\")\n\n# Save test data\nos.makedirs('./lazydata/test', exist_ok=True)\nos.makedirs('./lazydata/test/X', exist_ok=True)\nfor i in range(num_test):\n    os.makedirs('./lazydata/test/X/{}'.format(i), exist_ok=True)\n    # rgb\n    os.makedirs('./lazydata/test/X/{}/rgb'.format(i), exist_ok=True)\n    for j in range(3):\n        save_image(testx[0][i][j]/255, './lazydata/test/X/{}/rgb/{}.png'.format(i, j))\n    # depth\n    depth = testx[1][i].numpy()\n    np.save('./lazydata/test/X/{}/depth.npy'.format(i), depth)\n    # field id\n    pkl.dump(testx[2][i], open('./lazydata/test/X/{}/field_id.pkl'.format(i), 'wb'))\n\nprint(\"Saved test data\")","metadata":{"execution":{"iopub.status.busy":"2022-12-14T18:34:03.122340Z","iopub.execute_input":"2022-12-14T18:34:03.122760Z","iopub.status.idle":"2022-12-14T18:42:11.121136Z","shell.execute_reply.started":"2022-12-14T18:34:03.122727Z","shell.execute_reply":"2022-12-14T18:42:11.117931Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Saved train data\nSaved test data\n","output_type":"stream"}]},{"cell_type":"code","source":"transformations = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset=LazyLoadDataset(\"/kaggle/working/lazydata/\", transform = transformations)\ntest_dataset = LazyLoadDataset(\"/kaggle/working/lazydata/\", transform = transformations, train=False)\n\ntrain_dataset.__len__()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"4Aj6uS-ZoHN6","outputId":"7ceb590a-6593-4539-febc-3524ffa1ecfe","execution":{"iopub.status.busy":"2022-12-14T18:43:55.261990Z","iopub.execute_input":"2022-12-14T18:43:55.262799Z","iopub.status.idle":"2022-12-14T18:43:55.285218Z","shell.execute_reply.started":"2022-12-14T18:43:55.262747Z","shell.execute_reply":"2022-12-14T18:43:55.284149Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"3396"},"metadata":{}}]},{"cell_type":"code","source":"#we add it to a dataloader for our model\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"id":"TgQfBXgcoK4M","execution":{"iopub.status.busy":"2022-12-14T18:43:56.579726Z","iopub.execute_input":"2022-12-14T18:43:56.580257Z","iopub.status.idle":"2022-12-14T18:43:56.588036Z","shell.execute_reply.started":"2022-12-14T18:43:56.580208Z","shell.execute_reply":"2022-12-14T18:43:56.586577Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We check dimensions\n","metadata":{"id":"8FiMOEReyHVZ"}},{"cell_type":"code","source":"(img0, img1, img2, depth, field_id), Y = train_dataset[0]\nimg0.shape, img1.shape, img2.shape, depth.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzCKxWkexBpb","outputId":"5380f248-7394-4a2b-dbc8-595f4528e0a5","execution":{"iopub.status.busy":"2022-12-14T18:43:58.690907Z","iopub.execute_input":"2022-12-14T18:43:58.691580Z","iopub.status.idle":"2022-12-14T18:43:58.757294Z","shell.execute_reply.started":"2022-12-14T18:43:58.691526Z","shell.execute_reply":"2022-12-14T18:43:58.755756Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((224, 224, 3), (224, 224, 3), (224, 224, 3), (3, 224, 224))"},"metadata":{}}]},{"cell_type":"code","source":"(img0, img1, img2, depth, field_id) = test_dataset[0]\nimg0.shape, img1.shape, img2.shape, depth.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-14T18:43:59.892466Z","iopub.execute_input":"2022-12-14T18:43:59.892912Z","iopub.status.idle":"2022-12-14T18:43:59.914949Z","shell.execute_reply.started":"2022-12-14T18:43:59.892879Z","shell.execute_reply":"2022-12-14T18:43:59.913971Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((224, 224, 3), (224, 224, 3), (224, 224, 3), (3, 224, 224))"},"metadata":{}}]},{"cell_type":"markdown","source":"Lets make sure we mark our device to cuda\n","metadata":{"id":"d4di407MwxDj"}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"FUiV5BbswzQb","execution":{"iopub.status.busy":"2022-12-14T18:44:01.460982Z","iopub.execute_input":"2022-12-14T18:44:01.461392Z","iopub.status.idle":"2022-12-14T18:44:01.467611Z","shell.execute_reply.started":"2022-12-14T18:44:01.461353Z","shell.execute_reply":"2022-12-14T18:44:01.466589Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T18:44:02.993204Z","iopub.execute_input":"2022-12-14T18:44:02.993861Z","iopub.status.idle":"2022-12-14T18:44:03.002970Z","shell.execute_reply.started":"2022-12-14T18:44:02.993808Z","shell.execute_reply":"2022-12-14T18:44:03.001716Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"54"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train function\n\n","metadata":{"id":"fh6FopK8w7P8"}},{"cell_type":"code","source":"#create a helper function to calculate Root Mean Square Error for the competition\n\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))\n\n\ndef train(epoch, model, optimizer, permute_pixels=None, permutation_order=None):\n    \"\"\"\n    Train the model for one epoch\n    Args:\n        epoch (int): current epoch\n        model (nn.Module): model to train\n        optimizer (torch.optim): optimizer to use\n        permute_pixels (function): function to permute the pixels (default: None)\n        permutation_order (1D torch array): order of the permutation (default: None)\n    \"\"\"\n\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n\n        #shape of data is [64,224,224,9] target is [64,12]\n        #now, we want to include depth as well, so we will have to make some tricks\n        #reshape data from [64,224,224,9] to [64,9,224,224]\n\n        #lets change some things inside-----------------------------------\n        RGBs=torch.stack((data[0][:,:,:,0],data[0][:,:,:,1],data[0][:,:,:,2]),1)\n        #concate depth\n        #data=torch.cat((RGBs,data[3]), 1)\n        \n        data = RGBs\n\n        #now data is [64,12,224,224], which matches with the target [64,12], and has the dim 224x224 per image\n\n        #------------------------------------------------------------------\n        \n        data, target = data.to(device), target.to(device)\n        \n        #if we want to permute pixels\n        if permute_pixels is not None:\n            data = permute_pixels(data.squeeze(),permutation_order)\n\n        output = model(data)\n        rmse_loss = RMSELoss()\n        loss = rmse_loss(output.float(), target.float())\n\n        optimizer.zero_grad()#reset gradients\n        loss.backward()#calculate gradients\n        optimizer.step()#update \n\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))","metadata":{"id":"Kic5bxaQoN-Q","execution":{"iopub.status.busy":"2022-12-14T18:44:04.433653Z","iopub.execute_input":"2022-12-14T18:44:04.434321Z","iopub.status.idle":"2022-12-14T18:44:04.448479Z","shell.execute_reply.started":"2022-12-14T18:44:04.434283Z","shell.execute_reply":"2022-12-14T18:44:04.447201Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# CNN Arquitecture","metadata":{"id":"S2pXV1NTyMEL"}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n        super(CNN, self).__init__()\n        \n        self.cnn_layers = nn.Sequential(\n            nn.BatchNorm2d(3, affine=True),\n            nn.Conv2d(in_channels=3, out_channels=conv_feature, kernel_size=5),#[12,224,224]-->[72,220,220], stride default value is 1\n            nn.LeakyReLU(0.2),\n            nn.MaxPool2d(kernel_size=2,stride=2),#20*20 shape of kernel, stride Default value is kernel_size.[72,220,220]--->[72,110,110]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3), #[72,110,110]--->[72,108,108]\n            nn.LeakyReLU(0.2),\n            nn.BatchNorm2d(conv_features, affine=True),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3), #[72,108,108]--->[72,106,106]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,106,106]--->[72,104,104]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,104,104]--->[72,102,102]\n            nn.LeakyReLU(0.2),\n            nn.BatchNorm2d(conv_features, affine=True),\n            nn.MaxPool2d(kernel_size=2,stride=2),#[72,104,104]--->[72,52,52]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,52,52]--->[72,50,50]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,104,104]--->[72,102,102]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,104,104]--->[72,102,102]\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3),#[72,104,104]--->[72,102,102],\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(in_channels=conv_feature, out_channels=conv_feature, kernel_size=3)#[72,104,104]--->[72,102,102]\n        )\n        \n        self.linear_layers=nn.Sequential(\n            torch.nn.Linear(conv_feature*41*41, 6096),\n            torch.nn.LeakyReLU(0.2),\n            torch.nn.Dropout(p=0.4),\n            torch.nn.Linear(6096, 6096),\n            torch.nn.LeakyReLU(0.2),\n            torch.nn.Dropout(p=0.4),\n            torch.nn.Linear(6096, output_size),\n        )\n        \n    def forward(self, x):\n        x = self.cnn_layers(x)\n        x = x.view(x.size(0), -1)\n        result = self.linear_layers(x)\n        return result","metadata":{"id":"mnEQTwmioTQU","execution":{"iopub.status.busy":"2022-12-14T20:00:34.219402Z","iopub.execute_input":"2022-12-14T20:00:34.219947Z","iopub.status.idle":"2022-12-14T20:00:34.236780Z","shell.execute_reply.started":"2022-12-14T20:00:34.219907Z","shell.execute_reply":"2022-12-14T20:00:34.235150Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":" # Train and see loss\n ","metadata":{"id":"WgbV57T3Diwn"}},{"cell_type":"code","source":"# Training settings \nconv_features = 32 # number of feature maps used to be 52\nfc_features = 50\noutput_size = 12\n\n#regular\n\nmodel_cnn = CNN(224*224,conv_features,fc_features,output_size) # create CNN model\n\n\nmodel_cnn.to(device)\n\n#changing learning rate\n\noptimizer = torch.optim.SGD(model_cnn.parameters(), lr=0.1, momentum=0.85) # use SGD with learning rate 0.01 and momentum 0.5\n# print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n\n#otherwise try adam as optimizer\n\ntest_accuracy = []\nfor epoch in range(0, 5):\n    train(epoch, model_cnn, optimizer)\n\n    \n    \n#     test_accuracy.append(test(model_cnn))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"eZkOjkH_oTSZ","outputId":"17904345-be75-4dbb-a2cb-9cef28bb5a5d","execution":{"iopub.status.busy":"2022-12-14T20:00:37.677639Z","iopub.execute_input":"2022-12-14T20:00:37.678250Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.067079\nTrain Epoch: 0 [640/3396 (19%)]\tLoss: 0.026059\nTrain Epoch: 0 [1280/3396 (37%)]\tLoss: 0.021959\nTrain Epoch: 0 [1920/3396 (56%)]\tLoss: 0.016068\nTrain Epoch: 0 [2560/3396 (74%)]\tLoss: 0.010961\nTrain Epoch: 0 [3200/3396 (93%)]\tLoss: 0.009723\nTrain Epoch: 1 [0/3396 (0%)]\tLoss: 0.012516\nTrain Epoch: 1 [640/3396 (19%)]\tLoss: 0.008985\nTrain Epoch: 1 [1280/3396 (37%)]\tLoss: 0.007751\nTrain Epoch: 1 [1920/3396 (56%)]\tLoss: 0.007651\nTrain Epoch: 1 [2560/3396 (74%)]\tLoss: 0.008772\nTrain Epoch: 1 [3200/3396 (93%)]\tLoss: 0.006298\nTrain Epoch: 2 [0/3396 (0%)]\tLoss: 0.007294\nTrain Epoch: 2 [640/3396 (19%)]\tLoss: 0.006983\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model_cnn.state_dict(), \"model_cnn_capstone.pt\")","metadata":{"id":"HTAYHgpgZoWG","execution":{"iopub.status.busy":"2022-12-14T19:51:31.968223Z","iopub.execute_input":"2022-12-14T19:51:31.970229Z","iopub.status.idle":"2022-12-14T19:51:36.581947Z","shell.execute_reply.started":"2022-12-14T19:51:31.970161Z","shell.execute_reply":"2022-12-14T19:51:36.580602Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model_cnn.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDSwLIFum6PK","outputId":"56a1bdc0-6237-499f-f0e8-97070ec0ddcb","execution":{"iopub.status.busy":"2022-12-14T19:51:37.879281Z","iopub.execute_input":"2022-12-14T19:51:37.880138Z","iopub.status.idle":"2022-12-14T19:51:37.888657Z","shell.execute_reply.started":"2022-12-14T19:51:37.880077Z","shell.execute_reply":"2022-12-14T19:51:37.887395Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (cnn_layers): Sequential(\n    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n    (1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n    (2): LeakyReLU(negative_slope=0.2)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (6): LeakyReLU(negative_slope=0.2)\n    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (9): LeakyReLU(negative_slope=0.2)\n    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (11): LeakyReLU(negative_slope=0.2)\n    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (13): LeakyReLU(negative_slope=0.2)\n    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (15): LeakyReLU(negative_slope=0.2)\n    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (17): LeakyReLU(negative_slope=0.2)\n    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (19): LeakyReLU(negative_slope=0.2)\n    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=64800, out_features=6096, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=6096, out_features=6096, bias=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=6096, out_features=12, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Training settings \nconv_features = 32 # number of feature maps\nfc_features = 50\noutput_size = 12\n\nmodel_cnn = CNN(224*224,conv_features,fc_features,output_size) # create CNN model\n#model_cnn = VGG16(num_classes= 12)\n\nmodel_cnn.to(device)\nmodel = model_cnn\nmodel.load_state_dict(torch.load('model_cnn_capstone.pt'))\nmodel.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlmw7R_Hm6R5","outputId":"a71480fd-1c23-417f-c073-41fdcc98e0d1","execution":{"iopub.status.busy":"2022-12-14T19:51:50.209223Z","iopub.execute_input":"2022-12-14T19:51:50.210591Z","iopub.status.idle":"2022-12-14T19:52:00.319682Z","shell.execute_reply.started":"2022-12-14T19:51:50.210547Z","shell.execute_reply":"2022-12-14T19:52:00.318023Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (cnn_layers): Sequential(\n    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n    (1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n    (2): LeakyReLU(negative_slope=0.2)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (6): LeakyReLU(negative_slope=0.2)\n    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (9): LeakyReLU(negative_slope=0.2)\n    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (11): LeakyReLU(negative_slope=0.2)\n    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (13): LeakyReLU(negative_slope=0.2)\n    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (15): LeakyReLU(negative_slope=0.2)\n    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (17): LeakyReLU(negative_slope=0.2)\n    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n    (19): LeakyReLU(negative_slope=0.2)\n    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=64800, out_features=6096, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=6096, out_features=6096, bias=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=6096, out_features=12, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"   # Now, lets model predictions for test and submit","metadata":{"id":"1caswfa6Dne-"}},{"cell_type":"code","source":"ids = []\nimg_data = []\nfor batch_idx, data in enumerate(test_loader):\n        ids.extend(data[4].tolist())\n        RGBs=torch.stack((data[0][:,:,:,0],data[0][:,:,:,1],data[0][:,:,:,2]),1)\n        #concate depth\n        #data=torch.cat((RGBs,data[3]),1)\n        data = RGBs\n        img_data.append(data)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T19:52:04.314815Z","iopub.execute_input":"2022-12-14T19:52:04.315248Z","iopub.status.idle":"2022-12-14T19:52:21.167003Z","shell.execute_reply.started":"2022-12-14T19:52:04.315214Z","shell.execute_reply":"2022-12-14T19:52:21.165691Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor data in img_data:\n    # Please remember to modify this loop, input and output based on your model/architecture\n    output = model_cnn(data)\n    preds.extend(output.cpu().detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2022-12-14T19:52:23.940194Z","iopub.execute_input":"2022-12-14T19:52:23.940837Z","iopub.status.idle":"2022-12-14T19:53:15.528987Z","shell.execute_reply.started":"2022-12-14T19:52:23.940804Z","shell.execute_reply":"2022-12-14T19:53:15.528027Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport pandas as pd\n\noutfile = 'submission.csv'\n\noutput_file = open(outfile, 'w')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T19:53:36.014256Z","iopub.execute_input":"2022-12-14T19:53:36.014724Z","iopub.status.idle":"2022-12-14T19:53:36.024067Z","shell.execute_reply.started":"2022-12-14T19:53:36.014691Z","shell.execute_reply":"2022-12-14T19:53:36.022700Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntitles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\ndf = pd.concat([pd.DataFrame(ids), pd.DataFrame(preds)], axis = 1, names = titles)\ndf.columns = titles\ndf.to_csv(outfile, index = False)\nprint(\"Written to csv file {}\".format(outfile))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T19:53:37.476550Z","iopub.execute_input":"2022-12-14T19:53:37.476961Z","iopub.status.idle":"2022-12-14T19:53:37.534364Z","shell.execute_reply.started":"2022-12-14T19:53:37.476931Z","shell.execute_reply":"2022-12-14T19:53:37.533020Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Written to csv file submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LazyLoadDataset_test(Dataset):\n    def __init__(self,path,train=True,transform=None):\n        self.transform=transform\n        path=path+(\"train/\" if train else \"test/\")\n        \n        self.pathX=path+\"X/\"\n        #self.pathY=path+\"Y/\"\n        \n        self.data=os.listdir(self.pathX)\n        \n    def __getitem__(self,idx):\n        f=self.data[idx]\n        \n        #X\n        #read rgb images\n        img0=cv2.imread(self.pathX+f+\"/rgb/0.png\")\n        img1=cv2.imread(self.pathX+f+\"/rgb/1.png\")\n        img2=cv2.imread(self.pathX+f+\"/rgb/2.png\")\n        if self.transform is not None:\n            img0=self.transform(img0)\n            img1=self.transform(img1)\n            img2=self.transform(img2)\n        #read image depth\n        depth=np.load(self.pathX+f+\"/depth.npy\")\n        #read field ID\n        field_id=pkl.load(open(self.pathX+f+\"/field_id.pkl\",\"rb\"))\n        #Y\n        #Y=np.load(self.pathY+f+\".npy\")\n        \n        #normalize rgb 0-255\n        img0=cv2.normalize(img0, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        img1=cv2.normalize(img1, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        img2=cv2.normalize(img2, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        depth=cv2.normalize(depth/1000, None, alpha=0, beta=1,\n                             norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)   \n        return (img0,img1,img2,depth,field_id)\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T21:52:02.485706Z","iopub.execute_input":"2022-12-13T21:52:02.486217Z","iopub.status.idle":"2022-12-13T21:52:02.498119Z","shell.execute_reply.started":"2022-12-13T21:52:02.486177Z","shell.execute_reply":"2022-12-13T21:52:02.496783Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"test_dataset=LazyLoadDataset_test(\"/kaggle/working/lazydata/\", train = False)\n#train_dataset=LazyLoadDataset(\"/kaggle/input/final-project-lazyload-data/lazydata/\")\n\ntest_dataset.__len__()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T22:09:54.880970Z","iopub.execute_input":"2022-12-13T22:09:54.881990Z","iopub.status.idle":"2022-12-13T22:09:54.892958Z","shell.execute_reply.started":"2022-12-13T22:09:54.881948Z","shell.execute_reply":"2022-12-13T22:09:54.891681Z"},"trusted":true},"execution_count":146,"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"849"},"metadata":{}}]},{"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T22:09:56.140147Z","iopub.execute_input":"2022-12-13T22:09:56.140923Z","iopub.status.idle":"2022-12-13T22:09:56.150661Z","shell.execute_reply.started":"2022-12-13T22:09:56.140881Z","shell.execute_reply":"2022-12-13T22:09:56.149446Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport pandas as pd\n\noutfile = 'submission.csv'\n\noutput_file = open(outfile, 'w')\n\ntitles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\npreds = []\n\n#test_data = torch.load('/kaggle/input/csci-ua-473-intro-to-machine-learning-fall22/test/test/testX.pt')\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T21:52:09.083637Z","iopub.execute_input":"2022-12-13T21:52:09.084041Z","iopub.status.idle":"2022-12-13T21:52:09.094217Z","shell.execute_reply.started":"2022-12-13T21:52:09.084007Z","shell.execute_reply":"2022-12-13T21:52:09.092966Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (cnn_layers): Sequential(\n    (0): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n    (1): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): ReLU()\n    (5): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1))\n    (6): ReLU()\n    (7): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1))\n    (8): ReLU()\n    (9): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1))\n    (10): ReLU()\n    (11): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1))\n    (12): ReLU()\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): ReLU()\n    (15): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (linear_layers): Sequential(\n    (0): Linear(in_features=57624, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=12, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"preds = []\nfor data in test_loader:\n    # Please remember to modify this loop, input and output based on your model/architecture\n    \n    #lets change some things inside-----------------------------------\n    RGBs=torch.stack((data[0][:,:,:,0],data[0][:,:,:,1],data[0][:,:,:,2],\n                        data[1][:,:,:,0],data[1][:,:,:,1],data[1][:,:,:,2],\n                          data[2][:,:,:,0],data[2][:,:,:,1],data[2][:,:,:,2]),1)\n    #concate depth\n    \n    file_ids = data[4]\n\n    data_ = torch.cat((RGBs,data[3]), 1)\n    \n\n    #now data is [64,12,224,224], which matches with the target [64,12], and has the dim 224x224 per image\n\n    #------------------------------------------------------------------\n    \n    output = model_cnn(data_)\n    preds.append(output.cpu().detach().numpy()/1000)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T22:09:58.963927Z","iopub.execute_input":"2022-12-13T22:09:58.964310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_ids","metadata":{"execution":{"iopub.status.busy":"2022-12-13T21:59:00.592375Z","iopub.execute_input":"2022-12-13T21:59:00.593044Z","iopub.status.idle":"2022-12-13T21:59:00.599634Z","shell.execute_reply.started":"2022-12-13T21:59:00.593010Z","shell.execute_reply":"2022-12-13T21:59:00.598573Z"},"trusted":true},"execution_count":145,"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"('3542',\n '3095',\n '571',\n '3198',\n '2620',\n '1584',\n '477',\n '2799',\n '4085',\n '2805',\n '692',\n '4167',\n '789',\n '3022',\n '736',\n '1438',\n '2749')"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(np.concatenate(preds)/100)], axis = 1, names = titles)\ndf.columns = titles\ndf.to_csv(outfile, index = False)\nprint(\"Written to csv file {}\".format(outfile))","metadata":{"execution":{"iopub.status.busy":"2022-12-13T21:54:01.908442Z","iopub.execute_input":"2022-12-13T21:54:01.909257Z","iopub.status.idle":"2022-12-13T21:54:01.959080Z","shell.execute_reply.started":"2022-12-13T21:54:01.909209Z","shell.execute_reply":"2022-12-13T21:54:01.958081Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"Written to csv file submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"-gxW6r8G1mBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"4Z7vNPge1mEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"PaFhRVa41mKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DkhtIrYQ1mMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"EpdzhzMb1mSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"BXn6QAK_otoR"},"execution_count":null,"outputs":[]}]}